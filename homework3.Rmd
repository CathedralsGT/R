---
title: "homework3"
author: "Qiyao Jiang"
date: "2025-06-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The following contains a line of code setting working directory, if that does not fit to your computer, just remember to change your working directory to this R directory

```{r set_working_directory}
getwd()
```

nlmfit_default uses default value a = 0.125, \_2 uses a = 0.1, \_3 uses a = 0.15

```{r load_data}
library(tidyverse)
gmp <- read.table("data/gmp.dat")
gmp <- gmp |>
  mutate(
    pop = gmp/pcgmp, 
    nlmfit_default = 6611*pop^(1/8),
    nlmfit_2 = 6611*pop^(0.1),
    nlmfit_3 = 6611*pop^(0.15),
    nlmfit_4 = 6611*pop^(0.1263177)
    )
gmp |>
  pivot_longer(
    cols = c("nlmfit_default", "nlmfit_2", "nlmfit_3", "nlmfit_4"),
    names_to = "pred_model", 
    values_to = "pcgmp_pred"
  ) -> new_gmp
glimpse(gmp)
head(gmp, 10)
tail(gmp, 10)
any(is.na(gmp))
```

```{r plot}
new_gmp |>
  ggplot() +
  geom_point(aes(x=pop, y=pcgmp), col="pink") +
  geom_line(aes(x=pop, y=pcgmp_pred, col=pred_model)) +
  scale_x_log10() +
  labs(x="pop", y="GMP per Cap", title="US Metropolitan Areas, 2006") +
  theme_bw() +
  theme(legend.title = element_blank())
```

```{r func_mse}
mse <- function(coefficient_vec, pop=gmp$pop, pcgmp=gmp$pcgmp) {
  y_0 <- coefficient_vec[1]
  a <- coefficient_vec[2]
  # stopifnot(is.numeric(coefficient_vec) && y_0 > 0 && a > 0)
  pred_pcgmp <- y_0*pop^a
  result <- mean((pred_pcgmp - pcgmp)^2)
  return(result)
}
mse(c(6611, 0.15))
mse(c(5000, 0.10))

```

```{r nlm_optimization}
suppressWarnings(nlm(mse, c(6611, 0.1)))
```

\$minimum represents the minimum value of our function mse() that the optimizer nlm can find given the initial coefficient;

\$estimate represents the corresponding value of the coefficient(y_0&a) when mse() reaches the minimum shown above

```{r func_plm}
plm <- function(y_0, a, pop=gmp$pop, pcgmp=gmp$pcgmp) {
  result = nlm(mse, c(y_0, a), pop=pop, pcgmp=pcgmp)
  return(result$estimate)
}
suppressWarnings(plm(y_0=6611, a=0.15))
suppressWarnings(plm(y_0=5000, a=0.10))
nlm(mse, c(6611, 0.15))$minimum < nlm(mse, c(5000, 0.10))$minimum
```

optimization algorithm nlm() may find different local minima when given different initial values;

the former one "y_0=6611, a=0.126" yields a lower MSE

```{r}
mean_pcgmp <- mean(gmp$pcgmp, na.rm=TRUE)
var_pcgmp <- var(gmp$pcgmp, na.rm=TRUE)
sd_pcgmp <- sd(gmp$pcgmp, na.rm=TRUE)
mean_pcgmp
var_pcgmp
sd_pcgmp
```

```{r func_jackknife}
jackknife <- function(i, vec=gmp$pcgmp) {
  result <- mean(vec[-i])
  return(result)
}
```

```{r jackknifed.means}
jackknifed.means <- function(vec=gmp$pcgmp){
  result <- rep(0, times=length(vec))
  for (i in 1:length(vec)) {
    result[i] <- jackknife(i, vec)
  }
  return(result)
}
```

```{r does_it_match}
jackknife_var <- function(vec=gmp$pcgmp) {
  return(var(jackknifed.means(vec))*((length(vec)-1)^2/length(vec)))
}
jackknife_sd <- sqrt(jackknife_var())
jackknife_var()*nrow(gmp)
jackknife_sd*sqrt(nrow(gmp))
```

we can see that the variance and standard deviation calculated by jackknife is very close(notice there's no difference under the accuracy above) to real variance and sd of the original data gmp\$pcgmp

```{r}
plm.jackknife <- function(y_0, a, pop=gmp$pop, pcgmp=gmp$pcgmp) {
  new_y0 = rep(0, times=length(pop))
  new_a = rep(0, times=length(pop))
  for (i in 1:length(pop)){
    new_y0[i] <- plm(y_0, a, pop=gmp$pop[-i], pcgmp=gmp$pcgmp[-i])[1]
    new_a[i] <- plm(y_0, a, pop=gmp$pop[-i], pcgmp=gmp$pcgmp[-i])[2]
  }
  jackknife_var(vec=new_y0) -> jk_pred_var_y0
  jackknife_var(vec=new_a) -> jk_pred_var_a
  return(c(jk_pred_var_y0, jk_pred_var_a))
}
suppressWarnings(plm.jackknife(y_0=6611, a=0.1))
```

```{r}
gmp2013 <- read.table("data/gmp-2013.dat", header=T)
gmp2013 |>
  mutate(
    pop = gmp/pcgmp
  ) ->gmp2013
glimpse(gmp2013)
suppressWarnings(plm(y_0=6611, a=0.1, pop=gmp2013$pop, pcgmp=gmp2013$pcgmp))
suppressWarnings(plm.jackknife(y_0=6611, a=0.1, pop=gmp2013$pop, pcgmp=gmp2013$pcgmp))
```

there is no significant difference.
